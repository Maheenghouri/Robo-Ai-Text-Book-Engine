---
title: "The Convergence: VLA"
sidebar_position: 2
---

# Vision-Language-Action (VLA) Models

## The Next Frontier
For decades, robotics and NLP (Natural Language Processing) were separate fields. Robots could walk but not understand; Chatbots could understand but not act.
**VLA (Vision-Language-Action)** models bridge this gap. They are Multimodal Large Language Models (MLLMs) trained not just on text and images, but on *robot actions*.

## Google's RT-2
Start-of-the-art models like Google's **RT-2** (Robotic Transformer 2) take an image and a text command as input, and output tokenized robot actions directly.
-   **Input**: Image of a table + Text "Pick up the extinct animal".
-   **Processing**: Recognizes a dinosaur toy.
-   **Output**: End-effector xyz coordinates and gripper open/close commands.

## Embodiment
This convergence gives AI a body. It moves us from "Artificial Intelligence" to **"Physical Intelligence"**.
