---
title: Module 4 Introduction
sidebar_position: 1
---

# Module 4: The Body (Vision-Language-Action)

## Overview
Welcome to the final frontier: **The Body**. This is where code meets reality in its most advanced form.

In this module, we explore the convergence of Large Language Models (LLMs) and Robotics, creating agents that can see, speak, think, and act.

## Curriculum
1. **[The Convergence](./01-llm-robotics-convergence.md)**: Understanding VLA models like RT-2.
2. **[Voice-to-Action](./02-voice-to-action-whisper.md)**: Giving the robot ears with OpenAI Whisper.
3. **[Cognitive Planning](./03-cognitive-planning.md)**: Using LLMs to break down complex goals into ROS 2 actions.
4. **[Capstone Project](./04-capstone-autonomous-humanoid.md)**: Building the full Autonomous Humanoid stack.

## Learning Objectives
By the end of this module, you will be able to:
- Explain what a VLA model is.
- Implement a voice command pipeline.
- Use an LLM as a high-level planner for a robot.
- Architect a complete "Physical AI" system.
